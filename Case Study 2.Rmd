---
title: "Case Study 2"
author: "Manuela Giansante & Lucia Camenisch"
date: "2023-03-27"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, fig.align = "center")
```

```{r message=FALSE, warning= FALSE}
library(data.table)
library(pastecs)
library(dplyr)
library(corrplot)
library(naniar)
library(psych)
library(rcompanion) 
library(kableExtra)
library(caret)
library(REdaS)
```

```{r}
descriptions<- fread("Data File_Case Study_Factor Analysis_Variables.csv")
df<-read.csv("Data File_Case_Study_Factor Analysis_MD.csv")
```

```{r}
df1<-df[,c(9:41)]
vis_miss(df1) #qd22,26,28,29 misses some data
df1<-na.omit(df1)# remove them as she said to do
vis_miss(df1)
```
- how many are the requited case

```{r}
raqMatrix <- cor(df1)
corrplot(as.matrix(raqMatrix))
```

####  check whether the data set and all of its variables are suitable for factor analysis.
A strong correlation between two variables is most likely a signal of redundancy.
In regards to the correlation plot above, there are not very clear patterns, but we can observe some variables are highly correlated between one another, which is good since we want to perform a factor analysis.
More precisely, we are interested in the underlying factors causing those variables to move the way that they move. A weaker correlation indicates that there are different factors affecting the variables, meaning they do not describe the same domain.

Listed below are the variables displaying higher correlation, with a selected threshold of 0.75.
```{r}
# which pairs?
high_cor<- findCorrelation(raqMatrix, cutoff = 0.75)
varnames <- colnames(df1)[high_cor]
pairs <- combn(varnames, 2)# to generate all possible pairs of the highly correlated variables
pairs <- t(pairs)
# you filter them
for (i in 1:nrow(pairs)) {
  corr <- cor(df1[,pairs[i,]])
  if (corr[1,2] > 0.75) {
    cat(paste(pairs[i,1], "and", pairs[i,2], "have a correlation of", corr[1,2], "\n"))
  }}
```

To make sure that our data is suitable for Factor Analysis, we want to test to what extent our variables are correlated to one another.
To further the analysis we measure the Sampling Adequacy based on the Kaiser-Meyer-Olking criterion:

```{r}
#Kaiser-Meyer-Olkin Statistics
KMOS(df1)
```
The KMO is a value that goes from 0 to 1, 1 indicating that all variables are suitable for factor analysis, 0 means that the correlations between the predictors are only due to the random error in the measurements.*idk if this sentence makes sense*
The measurement is computed with the following formula:
$MO_j=\frac{\sum_{i\neq j}r^2_{ij}}{\sum_{i \neq j} r^2_{ij}+\sum_{i\neq j}u}$
where $r_{ij}$ is the correlation matrix, $u_{ij}$is the partial covariance matrix.

Our KMO value amounts to 0.96 which satisfying (above 0.6), the variables are suitable for factor analysis. Qd4 displays a lower measure than all other variables, we must look out for it in the next steps, even if it is not included in the high correlation list.

```{r}
# Anti-Image correlation matrix
# we use the KMO function fro the psych package
KMO.df<-KMO(df1)$ImCov

ImCov_df <- as.data.frame(KMO.df)
row.names(ImCov_df) <- colnames(ImCov_df) <- colnames(df1)

# Highlight diagonal and add row/column names
kbl(ImCov_df, align = "c") %>%
  row_spec(0, bold = TRUE) %>%
  column_spec(0, bold = TRUE)
  mutate_all(funs(cell_spec(., bold = ifelse(row.names(ImCov_df) == colnames(ImCov_df), TRUE, FALSE))))

# Print anti-image correlation matrix as a table
ImCov_kbl
```



Another way to judge whether our data is suitable for factor analysis would be the Bartlett's test.The null hypothesis of this test is that the sample originates from a population, where all variables are uncorrelated, or in other words the correlation matrix equals the identity matrix.
The test is more suited for cases in which each variable has less than 5 observations, because it is very sensitive within large sample it might not be a good fit for us. 

```{r}
# l ho fatto ma possiamo anche toglierlo imo
#Bartlett's Test of Sphericity
bart_spher(df1)
```



#### Principal Component Analysis 1
```{r}
PC0 <- psych::principal(df1, 
                        rotate="varimax", scores=TRUE)

plot(PC0$values,xlab="Factor Number",ylab="Eigenvalue",main="Scree plot",cex.lab=1.2,cex.axis=1.2,cex.main=1.8)+abline(h=1)
```


```{r}
PC1 <- psych::principal(df1, 
                        rotate="varimax",
                        nfactors=7, 
                        scores=TRUE)
# i did 7 bc with 6 the loadings looked a mess
# i guess that the number of factors depends on how many dots are above the line
PC1_communalities=data.frame(sort(PC1$communality))
PC1_communalities %>% kbl()
```

#### total variance explained
*I honeslty have no clue where this is going sorry*
```{r}
EigenValue=PC1$values
Variance=EigenValue/ncol(subset(df1))*100
SumVariance=cumsum(EigenValue/ncol(subset(df1)))
Total_Variance_Explained=cbind(EigenValue=EigenValue[EigenValue>0],Variance=Variance[Variance>0],Total_Variance=SumVariance[Variance>0])
Total_Variance_Explained %>% kbl()
```


```{r}
print(PC1$loadings, cutoff=0.3,sort=TRUE)
# so here she looks at like the ones that are not in patterns
# than se confronts it with the communalities and from there she decides which ones to eliminate
```
qd26 low on rc7
qd7,qd8 low on rc5

#### principal analysis2 (for the last standing)
```{r}
# for now i put in just the ones that look out of pattern
PC2 <- psych::principal(subset(df1, select = c(qd1:qd6,qd9:qd25,qd27:qd33)), rotate="varimax", scores=TRUE)

plot(PC2$values,xlab="Factor Number",ylab="Eigenvalue",main="Scree plot",cex.lab=1.2,cex.axis=1.2,cex.main=1.8)+abline(h=1)
```


```{r}
#idk if we should changed the names
# maybe even find a function that does this on its own
EigenValue=PC2$values
Variance=EigenValue/ncol(subset(df1, select = c(qd1:qd6,qd9:qd25,qd27:qd33)))*100
SumVariance=cumsum(EigenValue/ncol(subset(df1, select = c(qd1:qd6,qd9:qd25,qd27:qd33))))
Total_Variance_Explained2=cbind(EigenValue=EigenValue[EigenValue>0],Variance=Variance[Variance>0],Total_Variance=SumVariance[Variance>0])
Total_Variance_Explained2
```
we try between 6 and 7 to see if the loadings look better
```{r}
PC3_6 <- psych::principal(subset(df1, select = c(qd1:qd6,qd9:qd25,qd27:qd33)), rotate="varimax",nfactors=6, scores=TRUE)
print(PC3_6$loadings, cutoff=0.3,sort=TRUE)
```
it does not look nice

```{r}
PC3_7 <- psych::principal(subset(df1, select = c(qd1:qd6,qd9:qd25,qd27:qd33)), rotate="varimax",nfactors=7, scores=TRUE)
print(PC3_7$loadings, cutoff=0.3,sort=TRUE)
```
*i honestly do not get why she did this, its literally pc2 called different, even tho for ours looks we might need to rethink how many we are taking away*
*do regression with factor scores???? do we even have a y?*


`

