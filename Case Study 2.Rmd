---
title: "Case Study 2"
author: "Manuela Giansante & Lucia Camenisch"
date: "2023-03-27"
output: html_document
editor_options: 
  markdown: 
    wrap: sentence
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, fig.align = "center")
```

```{r message=FALSE, warning= FALSE}
library(data.table)
library(pastecs)
library(dplyr)
library(corrplot)
library(naniar)
library(psych)
library(rcompanion) 
library(kableExtra)
library(knitr)
library(caret)
library(REdaS)
```

```{r}
descriptions<- fread("Data File_Case Study_Factor Analysis_Variables.csv")
df<-read.csv("Data File_Case_Study_Factor Analysis_MD.csv")
```

```{r}
df1<-df[,c(9:41)]
vis_miss(df1) #qd22,26,28,29 misses some data
df1<-na.omit(df1)# remove them as she said to do
vis_miss(df1)
```

-   how many are the requited case

### Exercise 1: Orthogonal Factor Analysis

```{r}
raqMatrix <- cor(df1)
corrplot(as.matrix(raqMatrix))
```

#### check whether the data set and all of its variables are suitable for factor analysis.

A strong correlation between two variables is most likely a signal of redundancy.
In regards to the correlation plot above, there are not very clear patterns, but we can observe some variables are highly correlated between one another, which is good since we want to perform a factor analysis.
More precisely, we are interested in the underlying factors causing those variables to move the way that they move.
A weaker correlation indicates that there are different factors affecting the variables, meaning they do not describe the same domain.

Listed below are the variables displaying higher correlation, with a selected threshold of 0.75.

```{r}
# which pairs?
high_cor<- findCorrelation(raqMatrix, cutoff = 0.75)
varnames <- colnames(df1)[high_cor]
pairs <- combn(varnames, 2)# to generate all possible pairs of the highly correlated variables
pairs <- t(pairs)
# you filter them
for (i in 1:nrow(pairs)) {
  corr <- cor(df1[,pairs[i,]])
  if (corr[1,2] > 0.75) {
    cat(paste(pairs[i,1], "and", pairs[i,2], "have a correlation of", corr[1,2], "\n"))
  }}
```

To make sure that our data is suitable for Factor Analysis, we want to test to what extent our variables are correlated to one another.
To further the analysis we measure the Sampling Adequacy based on the Kaiser-Meyer-Olking criterion:

```{r}
#Kaiser-Meyer-Olkin Statistics
KMOS(df1)
```

The KMO is a value that goes from 0 to 1, 1 indicating that all variables are suitable for factor analysis, 0 means that the correlations between the predictors are only due to the random error in the measurements.*idk if this sentence makes sense* The measurement is computed with the following formula: $MO_j=\frac{\sum_{i\neq j}r^2_{ij}}{\sum_{i \neq j} r^2_{ij}+\sum_{i\neq j}u}$ where $r_{ij}$ is the correlation matrix, $u_{ij}$is the partial covariance matrix.

Our KMO value amounts to 0.96 which satisfying (above 0.6), the variables are suitable for factor analysis.
Qd4 displays a lower measure than all other variables, we must look out for it in the next steps, even if it is not included in the high correlation list.

```{r}
# Anti-Image correlation matrix
# we use the KMO function from the psych package
KMO(df1)$ImCov %>% diag()%>% kbl()# only printing the diagonal
# maybe add rownames
# the results are very ugly, she doesnt do it in the exercise. let's consider not including it

```

Another way to judge whether our data is suitable for factor analysis would be the Bartlett's test.The null hypothesis of this test is that the sample originates from a population, where all variables are uncorrelated, or in other words the correlation matrix equals the identity matrix.
The test is more suited for cases in which each variable has less than 5 observations, because it is very sensitive within large sample it might not be a good fit for us.

```{r}
# l ho fatto ma possiamo anche toglierlo imo
#Bartlett's Test of Sphericity
bart_spher(df1)
```

The test results highly significant, we can reject the null hypothesis, the data is fit for factor analysis.

#### Principal Component Analysis 1

```{r}
PC0 <- psych::principal(df1, 
                        rotate="varimax", scores=TRUE)

plot(PC0$values,xlab="Factor Number",ylab="Eigenvalue",main="Scree plot",cex.lab=1.2,cex.axis=1.2,cex.main=1.8)+abline(h=1)
```

The Eigenvalues are plotted above, the horizontal line represents the significance threshold.
After the first value there is a very steep drop, followed by a slower decline.
The point above the significance level(and above the "elbow") are around 7 or 8.

We attempt at retaining 8 factors:

```{r}
PC1 <- psych::principal(df1, 
                        rotate="varimax",
                        nfactors=8, 
                        scores=TRUE)
# i did 7 bc with 6 the loadings looked a mess
# i guess that the number of factors depends on how many dots are above the line
PC1_communalities=data.frame(sort(PC1$communality))
PC1_communalities %>% kbl()
```

We print out the communalities (sum of squared factor loadings for each variable) for all the variables, sorted in ascending order.
The communality of a variable describes how much each factor, out of the designed 7, contributes/explains the variance of this variable.
So if the communality equals 1 then a complete replication of the variable can me obtained through the sole use of factors.
In our case all variables have communalities below 1, so there is part of the variance that cannot be explained/modelled through factors.
The lowest communalities are qd23,qd26,qd28.

As previously done, we utilized the "varimax" rotation, which is designed to simplify the interpretation of the loadings.
"Simplification of factors: maximize variance of squared factor loadings; when loadings close to 0 or close to 1, factor becomes easier to interpret."

#### total variance explained

```{r}
EigenValue=PC1$values
Variance=EigenValue/ncol(subset(df1))*100
SumVariance=cumsum(EigenValue/ncol(subset(df1)))
Total_Variance_Explained=cbind(EigenValue=EigenValue[EigenValue>0],Variance=Variance[Variance>0],Total_Variance=SumVariance[Variance>0])
Total_Variance_Explained 
```

Total Variance: $\begin{aligned}s^2_j&=1=h^2_j+c^2_j+e^2_j\\&=communality+specificity+measurament error\\&=h^2_j+d^2_j\\&=(1-d^2_j)+(c^2_j+e^2_j)\\&= communality+uniqueness\end{aligned}$ The 8 factors explain about 80% of the total variance in the data.

#### Loadings for all variables:

$Loadings= Eigenvectors\times\sqrt{Eigenvalues}$ The loadings measure the correlation between the variables and the factors.
Their signs indicated the direction of the relationship with the factors, the values the strength of such relationship.

```{r}
print(PC1$loadings, cutoff=0.3,sort=TRUE)
# so here she looks at like the ones that are not in patterns
# than se confronts it with the communalities and from there she decides which ones to eliminate
```

As anticipated by the communalites qd26 is low on rc3 (factor 3).
Further more, qd28 and qd8 are low on rc1.
The other display quite clear patterns *insert a summary of the "clusters"*

#### Principal Analysis with filtered variables:

```{r}
PC2 <- psych::principal(subset(df1, select = c(qd1:qd7,qd9:qd25,qd27,qd29:qd33)), rotate="varimax", scores=TRUE)

plot(PC2$values,xlab="Factor Number",ylab="Eigenvalue",main="Scree plot",cex.lab=1.2,cex.axis=1.2,cex.main=1.8)+abline(h=1)
```

The Scree plots suggests the same number of factors.

```{r}
#idk if we should changed the names
# maybe even find a function that does this on its own
EigenValue=PC2$values
Variance=EigenValue/ncol(subset(df1, select = c(qd1:qd7,qd9:qd25,qd27,qd29:qd33)))*100
SumVariance=cumsum(EigenValue/ncol(subset(df1, select = c(qd1:qd7,qd9:qd25,qd27,qd29:qd33))))
Total_Variance_Explained2=cbind(EigenValue=EigenValue[EigenValue>0],Variance=Variance[Variance>0],Total_Variance=SumVariance[Variance>0])
Total_Variance_Explained2
```

The variance explained has increased to 83%.

```{r}
PC3 <- psych::principal(subset(df1, select = c(qd1:qd7,qd9:qd25,qd27,qd29:qd33)), rotate="varimax",nfactors=8, scores=TRUE)
print(PC3$loadings, cutoff=0.3,sort=TRUE)
```

We try and see if removing dq6 and qd31 helps:

```{r}
PC3_new <- psych::principal(subset(df1, select = c(qd1:qd5,qd7,qd9:qd25,qd27,qd29:qd30,qd32,qd33)), rotate="varimax",nfactors=8, scores=TRUE)
print(PC3_new$loadings, cutoff=0.3,sort=TRUE)
```

It does not, our final selection is `c(qd1:qd7,qd9:qd25,qd27,qd29:qd33)`
*should add the whole discourse about what means what*

### Exercise 2:
